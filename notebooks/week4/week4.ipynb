{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "In this homework, we'll deploy the ride duration model in batch mode. Like in homework 1, we'll use the Yellow Taxi Trip Records dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Notebook\n",
    "\n",
    "We'll start with the same notebook we ended up with in homework 1.\n",
    "We cleaned it a little bit and kept only the scoring part. You can find the initial notebook [here](homework/starter.ipynb).\n",
    "\n",
    "Run this notebook for the March 2023 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open('model.bin', 'rb') as f_in:\n",
    "    dv, model = pickle.load(f_in)\n",
    "\n",
    "categorical = ['PULocationID', 'DOLocationID']\n",
    "\n",
    "def read_data(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df['duration'] = df.duration.dt.total_seconds() / 60\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "\n",
    "    return df\n",
    "\n",
    "df = read_data('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet')\n",
    "dicts = df[categorical].to_dict(orient='records')\n",
    "X_val = dv.transform(dicts)\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What's the standard deviation of the predicted duration for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.247"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(round(y_pred.std(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Preparing the output\n",
    "\n",
    "Like in the course videos, we want to prepare the dataframe with the output. \n",
    "\n",
    "First, let's create an artificial `ride_id` column:\n",
    "\n",
    "```python\n",
    "df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')\n",
    "```\n",
    "\n",
    "Next, write the ride id and the predictions to a dataframe with results. \n",
    "\n",
    "Save it as parquet:\n",
    "\n",
    "```python\n",
    "df_result.to_parquet(\n",
    "    output_file,\n",
    "    engine='pyarrow',\n",
    "    compression=None,\n",
    "    index=False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame()\n",
    "df_result['ride_id'] = f'{2023:04d}/{3:02d}_' + df.index.astype('str')\n",
    "df_result['prediction'] = y_pred\n",
    "\n",
    "output_file = 'output.parquet'\n",
    "df_result.to_parquet(\n",
    "    output_file,\n",
    "    engine='pyarrow',\n",
    "    compression=None,\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What's the size of the output file?\n",
    "\n",
    "__Note:__ Make sure you use the snippet above for saving the file. It should contain only these two columns. For this question, don't change the\n",
    "dtypes of the columns and use `pyarrow`, not `fastparquet`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66M\toutput.parquet\n"
     ]
    }
   ],
   "source": [
    "!du -h output.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q3. Creating the scoring script\n",
    "\n",
    "Now let's turn the notebook into a script. \n",
    "\n",
    "Which command you need to execute for that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook week4.ipynb to script\n",
      "[NbConvertApp] Writing 5624 bytes to week4.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script week4.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Virtual environment\n",
    "\n",
    "Now let's put everything into a virtual environment. We'll use pipenv for that.\n",
    "\n",
    "Install all the required libraries. Pay attention to the Scikit-Learn version: it should be the same as in the starter\n",
    "notebook.\n",
    "\n",
    "After installing the libraries, pipenv creates two files: `Pipfile`\n",
    "and `Pipfile.lock`. The `Pipfile.lock` file keeps the hashes of the\n",
    "dependencies we use for the virtual env.\n",
    "\n",
    "What's the first hash for the Scikit-Learn dependency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First hash for scikit-learn: sha256:057b991ac64b3e75c9c04b5f9395eaf19a6179244c089afdebaad98264bff37c\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('Pipfile.lock', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "scikit_learn_data = data['default']['scikit-learn']['hashes'][0]\n",
    "print(f\"First hash for scikit-learn: {scikit_learn_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Parametrize the script\n",
    "\n",
    "Let's now make the script configurable via CLI. We'll create two \n",
    "parameters: year and month.\n",
    "\n",
    "Run the script for April 2023. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the mean predicted duration?\n",
    "\n",
    "Hint: just add a print statement to your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean predicted duration: 14.292\n"
     ]
    }
   ],
   "source": [
    "!python week4.py 2023 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Docker container \n",
    "\n",
    "Finally, we'll package the script in the docker container. \n",
    "For that, you'll need to use a base image that we prepared. \n",
    "\n",
    "This is what the content of this image is:\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.10.13-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY [ \"model2.bin\", \"model.bin\" ]\n",
    "```\n",
    "\n",
    "Note: you don't need to run it. We have already done it.\n",
    "\n",
    "It is pushed to [`agrigorev/zoomcamp-model:mlops-2024-3.10.13-slim`](https://hub.docker.com/layers/agrigorev/zoomcamp-model/mlops-2024-3.10.13-slim/images/sha256-f54535b73a8c3ef91967d5588de57d4e251b22addcbbfb6e71304a91c1c7027f?context=repo),\n",
    "which you need to use as your base image.\n",
    "\n",
    "That is, your Dockerfile should start with:\n",
    "\n",
    "```dockerfile\n",
    "FROM agrigorev/zoomcamp-model:mlops-2024-3.10.13-slim\n",
    "\n",
    "# do stuff here\n",
    "```\n",
    "\n",
    "This image already has a pickle file with a dictionary vectorizer\n",
    "and a model. You will need to use them.\n",
    "\n",
    "Important: don't copy the model to the docker image. You will need\n",
    "to use the pickle file already in the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/0)  docker:default\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                          docker:default\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 276B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/agrigorev/zoomcamp-model:mlops  0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 276B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/agrigorev/zoomcamp-model:mlops  0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 276B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/agrigorev/zoomcamp-model:mlops  0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (2/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 276B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/agrigorev/zoomcamp-model:mlops  0.5s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (11/11) FINISHED                               docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 276B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/agrigorev/zoomcamp-model:mlops  0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/agrigorev/zoomcamp-model:mlops-2024-3.10.13-slim  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 90B                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/6] RUN pip install pipenv                                    0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/6] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/6] COPY [Pipfile, Pipfile.lock, ./]                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/6] RUN pipenv install --python 3.10 --system --deploy        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [6/6] COPY week4.py .                                           0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:dcc232670f5eb6991490c5190b8a9119ddb296e704d25  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/mlops-course-week4                      0.0s\n",
      "\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!docker build -t mlops-course-week4 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now run the script with docker. \n",
    "\n",
    "What's the mean predicted duration for May 2023?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean predicted duration: 0.192\n"
     ]
    }
   ],
   "source": [
    "!docker run -it --rm docker.io/library/mlops-course-week4 2023 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Q1. Notebook\n",
    "\n",
    "What's the standard deviation of the predicted duration for this dataset?\n",
    "\n",
    "`6.24`\n",
    "\n",
    "### Q2. Preparing the output\n",
    "\n",
    "What's the size of the output file?\n",
    "\n",
    "`66M`\n",
    "\n",
    "### Q3. Creating the scoring script\n",
    "\n",
    "Which command you need to execute for that?\n",
    "\n",
    "`jupyter nbconvert --to script week4.ipynb`\n",
    "\n",
    "### Q4. Virtual environment\n",
    "\n",
    "What's the first hash for the Scikit-Learn dependency?\n",
    "\n",
    "`sha256:057b991ac64b3e75c9c04b5f9395eaf19a6179244c089afdebaad98264bff37c`\n",
    "\n",
    "### Q5. Parametrize the script\n",
    "\n",
    "What's the mean predicted duration? \n",
    "\n",
    "`14.29`\n",
    "\n",
    "### Q6. Docker container\n",
    "\n",
    "What's the mean predicted duration for May 2023? \n",
    "\n",
    "`0.19`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
